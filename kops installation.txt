<<kops
Landmark Technologies 
Tel: +1 437 215 2483 
mylandmarktech@gailcom 
www.mylandmarktech.com 
kops
Setup Kubernetes (K8s) Cluster on AWS Using KOPS

#!/bin/bash
#1) Create Ubuntu EC2 instance

#2) install AWSCLI

 sudo apt update -y
 sudo apt install unzip wget -y
 sudo curl https://s3.amazonaws.com/aws-cli/awscli-bundle.zip -o awscli-bundle.zip
 sudo apt install unzip python -y
 sudo unzip awscli-bundle.zip
 sudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws
 
 
#3) Install kops on ubuntu instance:

 	#Install wget if not installed
 	sudo apt install wget -y
 	sudo wget https://github.com/kubernetes/kops/releases/download/v1.16.1/kops-linux-amd64
 	sudo chmod +x kops-linux-amd64
 	sudo mv kops-linux-amd64 /usr/local/bin/kops
 
#4) Install kubectl

 sudo curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
 sudo chmod +x ./kubectl
 sudo mv ./kubectl /usr/local/bin/kubectl
 aws s3 mb s3://nubonglegah.k8.local
 aws s3 ls

5) Create an IAM role from Consloe or CLI with below Policies.

	AmazonEC2FullAccess 
	AmazonS3FullAccess
	IAMFullAccess 
	AmazonVPCFullAccess


Then Attach IAM role to ubuntu server from Console Select KOPS Server --> Actions --> Instance Settings --> Attach/Replace IAM Role --> Select the role which
You Created. --> Save.



6) create an S3 bucket Execute below commond in KOPS Server use unique bucket name if you get bucket name exists error.

	aws s3 mb s3://nubonglegah.k8.local
	aws s3 ls
	
    ex:
	# S3 bucket name should be unique across AWS
	aws s3 mb s3://simonlegah.k8s.local    s3://nubong.k8s.local
     
	Expose environment variable:

    # Add env variables in bashrc
    vi .bashrc
	
	# Give Unique Name And S3 Bucket which you created.
	export NAME=s3://legah2045.k8.local
	export KOPS_STATE_STORE=s3://legah2045.k8.local
 
    source .bashrc
	
7) Create sshkeys before creating cluster

    ssh-keygen
 

8)Create kubernetes cluster definitions on S3 bucket

	kops create cluster --zones us-east-2c --networking weave --master-size t2.medium --master-count 1 --node-size t2.large --node-count=2 ${NAME}
	
	kops create cluster --zones us-east-1e,zones us-east-2c --networking weave --master-size t2.medium --master-count 2 --node-size t2.micro --node-count=2 ${NAME}

	kops create secret --name ${NAME} sshpublickey admin -i ~/.ssh/id_rsa.pub

9) Create kubernetes cluser

	 kops update cluster ${NAME} --yes

10) Validate your cluster(KOPS will take some time to create cluster ,Execute below commond after 3 or 4 mins)

	   kops validate cluster
 
11) To list nodes

	  kubectl get nodes 
  
  
  
To Delete Cluster

   kops delete cluster --name=${NAME} --state=${KOPS_STATE_STORE} --yes  
   
====================================================================================================


IF you wan to SSH to Kubernates Master or Nodes Created by KOPS. You can SSH From KOPS_Server

ssh  admin@<IPOrDNS>
it above command  is not working
then execute
ssh -i ~/.ssh/id_rsa admin@<IPOrDNS>




Helm installation
==================
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get > get_helm.sh

chmod 700 get_helm.sh

./get_helm.sh

kubectl -n kube-system create serviceaccount tiller

kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller

helm init --service-account tiller

kubectl get pods --namespace kube-system

helm repo update

helm install --name autoscaler stable/cluster-autoscaler --namespace kube-system --set autoDiscovery.clusterName=<YourEKSClusterName> --set awsRegion=us-west-2  --set rbac.create=true --set cloudProvider=aws --set sslCertPath=/etc/kubernetes/pki/ca.crt

kubectl run nginx --image=nginx --port=80 --replicas=50

kubectl delete deployment nginx


Useful Links
------------

https://github.com/kubernetes/autoscaler

https://helm.sh/docs/using_helm/#installing-helm

https://github.com/helm/helm/blob/master/docs/rbac.md




Configure a Metrics Server on our Cluster4??
===========================================
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml


https://github.com/LandmakTechnology/metric-server
git clone https://github.com/LandmakTechnology/metric-server
kubectl apply -f metric-server/metrics-server-deploy.yml
=====
$ kubectl apply -f metric-server/metrics-server-deploy.yml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created

kubernetes objects used in deploying metric server:
 1. RBAC = role base access control
    serviceaccount = metrics-server
    clusterrole [ pods/nodes = get/watch/list ]
    clusterrolebinding:

    role
    rolebinding
2. application [third party apps and addons]
   deployment
3. Service ServiceDiscovery
   service = service/metrics-server


NAME     CPU(cores)   MEMORY(bytes)
webapp   2m           145Mi

    resources:
      requests:
        memory: "128Mi"
        cpu: "500m"
      limits:
        memory: "128Mi"
        cpu: "500m"
 The pod will suffer from oomKilled

RBAC objects:
  serviceaccount metrics-server
     - user
     - groups
     - pods
  clusterrole
     - pods/nodes [get/watch/list]
  clusterrolebinding
     -
  rolebinding
Deployment with HPA
==================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hpadeployment
spec:
  replicas: 2
  selector:
    matchLabels:
      name: hpapod
  template:
    metadata:
      labels:
        name: hpapod
    spec:
      containers:
        - name: hpacontainer
          image: k8s.gcr.io/hpa-example
          ports:
          - name: http
            containerPort: 80
          resources:
            requests:
              cpu: "100m"
              memory: "64Mi"
            limits:
              cpu: "100m"
              memory: "256Mi"
---
apiVersion: v1
kind: Service
metadata:
  name: hpaclusterservice
  labels:
    name: hpaservice
spec:
  ports:
    - port: 80
      targetPort: 80
  selector:
    name: hpapod
  type: ClusterIP

# HPA For above deployment(hpadeployment)
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: hpadeploymentautoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hpadeployment
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50


kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
kubectl autoscale deployment hpadeployment --cpu-percent=50 --min=1 --max=10

-# Create temp POD using below command interatively and increase the
-# load on demo app by accessing the service.

kubectl run -i --tty load-generator --rm  --image=busybox /bin/sh

-# Access the service to increase the load.

while true; do wget -q -O- http://hpaclusterservice; done
